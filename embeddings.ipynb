{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Open the refined dataset",
   "id": "810bac7ca18adc4e"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T18:26:44.646421Z",
     "start_time": "2024-11-25T18:26:44.284735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   source_video  \\\n",
       "0  2e246b45e7dfba0a7cfb2beb557c40d81dc02c99.flv   \n",
       "1  1e2598afd4d6a8728d6c0076354477db59702a5a.flv   \n",
       "2  c6d6d37c73f364e3902407e1da07c8e354f66c13.flv   \n",
       "3  458db5aa227ae49ceb8bc1bed5f9cf5b4bed63f2.flv   \n",
       "4  09b682c899b0727e9990d8e347cdce3df7c5550e.flv   \n",
       "\n",
       "                                   target_video  source_start  source_end  \\\n",
       "0  3ed4f5c0eb04c94353594e8be1a72bcc657e27c7.flv          24.0        25.0   \n",
       "1                                           NaN         186.0       198.0   \n",
       "2                                           NaN         677.0       692.0   \n",
       "3  6d1466ebc4de7e5ddb229bde090b5c5acac15c0c.flv           8.0        10.0   \n",
       "4  d2015b438b70f022967713d6f977ebc67a16839e.flv          15.0        25.0   \n",
       "\n",
       "   target_start  target_end  group                                category  \\\n",
       "0          15.0        22.0  210.0                    maradona_hand_of_god   \n",
       "1           NaN         NaN    NaN            the_last_samurai_last_battle   \n",
       "2           NaN         NaN    NaN              president_obama_takes_oath   \n",
       "3           4.0         6.0    0.0                     baggio_penalty_1994   \n",
       "4         193.0       203.0   71.0  david_beckham_lights_the_olympic_torch   \n",
       "\n",
       "   is_duplicate  \n",
       "0          True  \n",
       "1         False  \n",
       "2         False  \n",
       "3          True  \n",
       "4          True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_video</th>\n",
       "      <th>target_video</th>\n",
       "      <th>source_start</th>\n",
       "      <th>source_end</th>\n",
       "      <th>target_start</th>\n",
       "      <th>target_end</th>\n",
       "      <th>group</th>\n",
       "      <th>category</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e246b45e7dfba0a7cfb2beb557c40d81dc02c99.flv</td>\n",
       "      <td>3ed4f5c0eb04c94353594e8be1a72bcc657e27c7.flv</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>maradona_hand_of_god</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1e2598afd4d6a8728d6c0076354477db59702a5a.flv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the_last_samurai_last_battle</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c6d6d37c73f364e3902407e1da07c8e354f66c13.flv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>677.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>president_obama_takes_oath</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>458db5aa227ae49ceb8bc1bed5f9cf5b4bed63f2.flv</td>\n",
       "      <td>6d1466ebc4de7e5ddb229bde090b5c5acac15c0c.flv</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>baggio_penalty_1994</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09b682c899b0727e9990d8e347cdce3df7c5550e.flv</td>\n",
       "      <td>d2015b438b70f022967713d6f977ebc67a16839e.flv</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>david_beckham_lights_the_olympic_torch</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the alexnet",
   "id": "992fb906245992a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:26:49.029562Z",
     "start_time": "2024-11-25T18:26:45.891597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\").to(device)\n",
    "conv_layers = nn.Sequential(*list(alexnet.features.children()))\n",
    "alexnet.eval()\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "b84bda847d94e38a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.pyenv/versions/.venv_ml/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions to retrieve video embeddings",
   "id": "4d53a5826232a9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:26:49.043255Z",
     "start_time": "2024-11-25T18:26:49.033565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "def retrieve_frames(video_path, start_time, end_time, fps=1):\n",
    "    \"\"\"\n",
    "    Retrieves frames from a video at a specified frame rate between a start and end time\n",
    "    and returns them as a list of torch tensors.\n",
    "\n",
    "    :param video_path: Path to the video file\n",
    "    :param start_time: Start time in seconds\n",
    "    :param end_time: End time in seconds\n",
    "    :param fps: Frame rate at which to extract frames (default is 2 fps)\n",
    "    :return: List of frames as NumPy ndarrays\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return torch.empty()\n",
    "\n",
    "    # Get video properties\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_duration = frame_count / video_fps\n",
    "\n",
    "    # Convert start and end times to frame numbers\n",
    "    start_frame = int(start_time * video_fps)\n",
    "    end_frame = int(end_time * video_fps)\n",
    "\n",
    "    # Set the video capture to the start frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    # Calculate the interval between frames to extract\n",
    "    interval = int(video_fps / fps)\n",
    "\n",
    "    # Extract frames\n",
    "    frames = []\n",
    "    frame_number = start_frame\n",
    "    while cap.isOpened() and frame_number <= end_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Append the frame to the list\n",
    "        frame = torch.from_numpy(frame).permute(2, 0, 1)/255\n",
    "        frames.append(transforms(frame))\n",
    "\n",
    "        # Move to the next frame to extract\n",
    "        frame_number += interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    frames = torch.stack(frames)\n",
    "    return frames.detach().cpu()\n",
    "\n",
    "class MaxPoolConvOutputs(nn.Module):\n",
    "    def __init__(self, conv_layers):\n",
    "        super(MaxPoolConvOutputs, self).__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.pool = nn.AdaptiveMaxPool2d((1, 1))  # Pool to a single value per channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Conv2d):  # Apply max pooling only on Conv2d outputs\n",
    "                pooled_output = self.pool(x).squeeze(-1).squeeze(-1)  # Remove spatial dims\n",
    "                outputs.append(pooled_output)\n",
    "        return outputs\n",
    "\n",
    "def get_frames_emb(video):\n",
    "    max_pool_extractor = MaxPoolConvOutputs(conv_layers)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_pooled_outputs = max_pool_extractor(video.to(device))\n",
    "\n",
    "    result = torch.cat(max_pooled_outputs, dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalize_frames(video):\n",
    "    # Step 1: Average across frames (mean along the first dimension)\n",
    "    avg_emb = torch.mean(video, dim=0)\n",
    "\n",
    "    # Step 2: Zero-mean normalization (subtract the mean of the vector)\n",
    "    mean_value = torch.mean(avg_emb)\n",
    "    zero_mean_emb = avg_emb - mean_value\n",
    "\n",
    "    # Step 3: ℓ2-normalization (normalize by the L2 norm)\n",
    "    l2_norm = torch.norm(zero_mean_emb, p=2)\n",
    "    l2_normalized_emb = zero_mean_emb / l2_norm\n",
    "\n",
    "    return l2_normalized_emb\n",
    "\n",
    "def create_video_embedding(category, name, start, end):\n",
    "    frames = retrieve_frames(f'core_dataset/core_dataset/{category}/{name}', start, end)\n",
    "    emb = get_frames_emb(frames)\n",
    "    emb = normalize_frames(emb)\n",
    "    return emb"
   ],
   "id": "7bbfd1e6552b8b60",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a dictionary to store the video embeddings",
   "id": "4d92d3e77ff3fe41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:26:49.810393Z",
     "start_time": "2024-11-25T18:26:49.443012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# videos = {}\n",
    "\n",
    "# with open('embeddings.pkl', 'rb') as f:\n",
    "#     videos = pickle.load(f)\n",
    "\n",
    "# with open('mistakes.pkl', 'rb') as f:\n",
    "#     mistakes = pickle.load(f)\n",
    "    \n",
    "videos = {}\n",
    "mistakes = []"
   ],
   "id": "478dd8c29facb6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the embeddings",
   "id": "88360ae65941de86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T19:02:44.098662Z",
     "start_time": "2024-11-25T18:54:51.428358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bar = tqdm(total=len(df))\n",
    "bar.reset()\n",
    "for i, raw in df.iterrows():\n",
    "    try:\n",
    "        name = f\"{raw['source_video']}_{raw['source_start']}_{raw['source_end']}\"\n",
    "        if name not in videos:\n",
    "            videos[name] = create_video_embedding(raw['category'], raw['source_video'], raw['source_start'], raw['source_end'])\n",
    "        if raw['is_duplicate']:\n",
    "            name = f\"{raw['target_video']}_{raw['target_start']}_{raw['target_end']}\"\n",
    "            if name not in videos:\n",
    "                videos[name] = create_video_embedding(raw['category'], raw['target_video'], raw['target_start'], raw['target_end'])\n",
    "        bar.update(1)\n",
    "    except:\n",
    "        print(i)\n",
    "        mistakes.append(i)\n",
    "    if i % 100 == 0:\n",
    "        with open('embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(videos, f)\n",
    "        with open('mistakes.pkl', 'wb') as f:\n",
    "            pickle.dump(mistakes, f)\n",
    "    if i >= 4000:\n",
    "        break"
   ],
   "id": "2a2fa71b26afc2f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2998/10396 [27:48<1:08:38,  1.80it/s]\n",
      " 10%|█         | 1084/10396 [00:02<00:20, 444.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1401/10396 [00:03<00:22, 400.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2900/10396 [00:07<00:17, 419.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3370/10396 [03:27<44:03,  2.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3583/10396 [04:57<34:04,  3.33it/s]  [NULL @ 0x5e87d8731880] Invalid NAL unit size (3295 > 3073).\n",
      "[NULL @ 0x5e87d8731880] missing picture in access unit with size 3077\n",
      "[h264 @ 0x5e87d77d0ac0] Invalid NAL unit size (3295 > 3073).\n",
      "[h264 @ 0x5e87d77d0ac0] Error splitting the input into NAL units.\n",
      "[NULL @ 0x5e87d8731880] Invalid NAL unit size (3295 > 3073).\n",
      "[NULL @ 0x5e87d8731880] missing picture in access unit with size 3077\n",
      "[NULL @ 0x5e87d8731880] Invalid NAL unit size (3295 > 3073).\n",
      "[NULL @ 0x5e87d8731880] missing picture in access unit with size 3077\n",
      "[h264 @ 0x5e87d6b2bf40] Invalid NAL unit size (3295 > 3073).\n",
      "[h264 @ 0x5e87d6b2bf40] Error splitting the input into NAL units.\n",
      " 36%|███▌      | 3765/10396 [06:34<13:52,  7.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3996/10396 [07:52<31:35,  3.38it/s]  "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the embeddings",
   "id": "b8ca202773750ac3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T19:02:54.110081Z",
     "start_time": "2024-11-25T19:02:53.832308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(videos, f)\n",
    "    \n",
    "with open('mistakes.pkl', 'wb') as f:\n",
    "    pickle.dump(mistakes, f)"
   ],
   "id": "7bf8bf7987384058",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
