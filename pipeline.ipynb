{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:36:07.566816Z",
     "start_time": "2024-11-27T18:36:07.564064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "import hashlib\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "7b5d8e9d43d6fa3e",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Qdrant init. Run the qdrant client first\n",
    "\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant"
   ],
   "id": "6eaf0fb7a8de787d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:49.432174Z",
     "start_time": "2024-11-27T18:39:49.310005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "video_emb_dim = 500\n",
    "distance = Distance.EUCLID\n",
    "\n",
    "client.delete_collection('video')\n",
    "client.create_collection(\n",
    "    collection_name=\"video\",\n",
    "    vectors_config=VectorParams(size=video_emb_dim, distance=distance)\n",
    ")\n",
    "\n",
    "THRESHOLD = 0.677"
   ],
   "id": "f375c23e728a6bc7",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Code for obtaining video vector representation from alexnet",
   "id": "ab39c6b9480c2c1c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:50.052739Z",
     "start_time": "2024-11-27T18:39:49.608788Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load AlexNet model and extract convolutional layers\n",
    "alexnet = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\").to(device)\n",
    "conv_layers = nn.Sequential(*list(alexnet.features.children()))\n",
    "alexnet.eval()\n",
    "\n",
    "# Define transformations\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])  "
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:50.098149Z",
     "start_time": "2024-11-27T18:39:50.061291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_frames(video_path, start_time, fps=1):\n",
    "    \"\"\"\n",
    "    Retrieves frames from a video at a specified frame rate between a start and end time\n",
    "    and returns them as a list of torch tensors.\n",
    "\n",
    "    :param video_path: Path to the video file\n",
    "    :param start_time: Start time in seconds\n",
    "    :param end_time: End time in seconds\n",
    "    :param fps: Frame rate at which to extract frames (default is 2 fps)\n",
    "    :return: List of frames as NumPy ndarrays\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return torch.empty()\n",
    "\n",
    "    # Get video properties\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_duration = frame_count / video_fps\n",
    "\n",
    "    # Convert start and end times to frame numbers\n",
    "    start_frame = int(start_time * video_fps)\n",
    "    end_frame = total_duration\n",
    "\n",
    "    # Set the video capture to the start frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    # Calculate the interval between frames to extract\n",
    "    interval = int(video_fps / fps)\n",
    "\n",
    "    # Extract frames\n",
    "    frames = []\n",
    "    frame_number = start_frame\n",
    "    while cap.isOpened() and frame_number <= end_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Append the frame to the list\n",
    "        frame = torch.from_numpy(frame).permute(2, 0, 1) / 255\n",
    "        frames.append(transforms(frame))\n",
    "\n",
    "        # Move to the next frame to extract\n",
    "        frame_number += interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    frames = torch.stack(frames)\n",
    "    return frames.detach().cpu()\n",
    "\n",
    "class MaxPoolConvOutputs(nn.Module):\n",
    "    def __init__(self, conv_layers):\n",
    "        super(MaxPoolConvOutputs, self).__init__()\n",
    "        self.conv_layers = conv_layers\n",
    "        self.pool = nn.AdaptiveMaxPool2d((1, 1))  # Pool to a single value per channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Conv2d):  # Apply max pooling only on Conv2d outputs\n",
    "                pooled_output = self.pool(x).squeeze(-1).squeeze(-1)  # Remove spatial dims\n",
    "                outputs.append(pooled_output)\n",
    "        return outputs\n",
    "\n",
    "def get_frames_emb(video):\n",
    "    max_pool_extractor = MaxPoolConvOutputs(conv_layers)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_pooled_outputs = max_pool_extractor(video.to(device))\n",
    "\n",
    "    result = torch.cat(max_pooled_outputs, dim=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize_frames(video):\n",
    "    # Step 1: Average across frames (mean along the first dimension)\n",
    "    avg_emb = torch.mean(video, dim=0)\n",
    "\n",
    "    # Step 2: Zero-mean normalization (subtract the mean of the vector)\n",
    "    mean_value = torch.mean(avg_emb)\n",
    "    zero_mean_emb = avg_emb - mean_value\n",
    "\n",
    "    # Step 3: â„“2-normalization (normalize by the L2 norm)\n",
    "    l2_norm = torch.norm(zero_mean_emb, p=2)\n",
    "    l2_normalized_emb = zero_mean_emb / l2_norm\n",
    "\n",
    "    return l2_normalized_emb\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1152, 1500)  # Assuming input images are 28x28\n",
    "        self.fc2 = nn.Linear(1500, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 500)  # Output embedding of size 500\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output embedding\n",
    "        x = F.normalize(x, p=2, dim=1)  # Normalize embeddings to have unit norm\n",
    "        return x\n",
    "\n",
    "base_model = EmbeddingNet().to(device)\n",
    "base_model.load_state_dict(torch.load('api/model_77.pt', map_location=device))\n",
    "base_model.eval()"
   ],
   "id": "d69dcc8ebbb486af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43142/2542842698.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  base_model.load_state_dict(torch.load('api/model_77.pt', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (fc1): Linear(in_features=1152, out_features=1500, bias=True)\n",
       "  (fc2): Linear(in_features=1500, out_features=1000, bias=True)\n",
       "  (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The main pipeline",
   "id": "54e3edc5fa0d698c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:50.184310Z",
     "start_time": "2024-11-27T18:39:50.179677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(video_file_path):\n",
    "    # Get video descriptor\n",
    "    frames = retrieve_frames(video_file_path, start_time=0, fps=2)\n",
    "    emb = get_frames_emb(frames)\n",
    "    embedding_tensor = normalize_frames(emb)\n",
    "    \n",
    "    # Project the video descriptor into the embedding space\n",
    "    with torch.no_grad():\n",
    "        vector = base_model(embedding_tensor.unsqueeze(0).to(device)).detach().cpu().numpy()[0]\n",
    "        \n",
    "    # Search for the closest video in a collection\n",
    "    search_result = client.query_points(\n",
    "        collection_name=\"video\",\n",
    "        query=vector,\n",
    "        with_payload=True,\n",
    "        limit=1\n",
    "    ).points\n",
    "    \n",
    "    # If the result is not empty, check for the threshold\n",
    "    if len(search_result) > 0:\n",
    "        name2 = search_result[0].payload['name']\n",
    "        if search_result[0].score < THRESHOLD:\n",
    "            print(f\"Duplicate found: {name2}, score: {search_result[0].score}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Duplicate not found, closest score with {name2}: {search_result[0].score}\")\n",
    "\n",
    "    # Insert a video if it is not a duplicate\n",
    "    current_timestamp = str(time.time())\n",
    "    hashed_hex = hashlib.sha256(current_timestamp.encode()).hexdigest()\n",
    "    hashed_id = int(hashed_hex, 16) % (2**64)\n",
    "    client.upsert(\n",
    "        collection_name=\"video\",\n",
    "        points=[PointStruct(id=hashed_id, vector=vector, payload={'name': f'video_{hashed_id}'})]\n",
    "    )\n",
    "    print(f\"New video uploaded: video_{hashed_id}\")\n",
    "    return False \n",
    "    "
   ],
   "id": "966aa25ca885d14a",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Insert a new video",
   "id": "33ca7f2a0a06268b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:50.985977Z",
     "start_time": "2024-11-27T18:39:50.951470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_path = \"test_data/orig.mp4\"\n",
    "main(video_path)"
   ],
   "id": "2cbef890aa570a32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New video uploaded: video_15791415937813033677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Insert a duplicate. This video is resized and in a lower resolution",
   "id": "8772c7cbae09c0c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:51.741969Z",
     "start_time": "2024-11-27T18:39:51.718914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_path = \"test_data/dup.mp4\"\n",
    "main(video_path)"
   ],
   "id": "77073de58d8e9261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate found: video_15791415937813033677, score: 0.48300344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Insert a negative video",
   "id": "46f1e55ba3384617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:39:52.791860Z",
     "start_time": "2024-11-27T18:39:52.750521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_path = \"test_data/negative.mp4\"\n",
    "main(video_path)"
   ],
   "id": "43c1100c5c3f3a5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate not found, closest score with video_15791415937813033677: 1.5475478\n",
      "New video uploaded: video_7238750216663548295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that videos embeddings are stored in a vector database. You may want to reset it with `client.delete_collection('video')`",
   "id": "48cbed7194e48ea9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
